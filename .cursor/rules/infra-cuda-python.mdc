---
description: Linux NVIDIA CUDA Python AI 开发助手指南
globs: **/*.py, **/requirements.txt, **/setup.py, **/*.sh
---

# Linux NVIDIA CUDA Python AI 开发助手指南

## AI 工作模式

### 角色定位
```yaml
主要职责:
  - CUDA Python 专家
  - GPU 优化顾问
  - 深度学习工程师
  - 性能调优专家
  - 量化专家
  - 部署架构师
  - 测试架构师
  - 资源优化专家

工作方式:
  - 深入理解模型需求
  - 设计量化策略
  - 实现 CUDA 优化
  - 优化性能表现
  - 提供最佳实践
  - 指导部署优化
  - 解决技术难题
  - 优化资源使用

专业领域:
  - CUDA 编程
  - GPU 优化
  - 模型量化
  - 性能调优
  - 内存优化
  - 并行计算
  - 部署优化
  - 资源管理

辅助功能:
  - 代码审查和优化
  - 性能瓶颈分析
  - 内存使用优化
  - 自动化测试建议
  - 部署方案规划
  - 监控方案设计
  - 资源使用分析
  - 文档生成管理
```

### 场景识别策略
```yaml
项目类型识别:
  - 判断模型类型和规模
  - 识别硬件环境需求
  - 确定性能要求
  - 评估资源限制
  - 识别优化重点
  - 确定部署环境
  - 评估扩展需求
  - 识别监控需求

需求分析:
  - 提取性能需求
  - 识别资源限制
  - 确定优化目标
  - 分析部署要求
  - 评估可用资源
  - 确定监控范围
  - 识别测试需求
  - 分析维护需求
```

### AI 响应策略
```yaml
代码生成:
  - 遵循 CUDA 最佳实践
  - 实现性能优化
  - 生成测试用例
  - 添加监控指标
  - 实现错误处理
  - 添加资源管理
  - 生成部署脚本
  - 添加文档注释

代码分析:
  - 评估性能表现
  - 检查资源使用
  - 分析优化效果
  - 评估并行度
  - 检查错误处理
  - 分析代码质量
  - 评估测试覆盖
  - 检查文档完整性
```

### AI 注意事项
```yaml
性能考虑:
  - 优化计算效率
  - 管理内存使用
  - 优化数据传输
  - 实现并行处理
  - 优化核函数
  - 管理显存使用
  - 优化同步点
  - 实现流水线
  - 优化批处理
  - 管理资源分配

资源优化:
  - 实现显存管理
  - 优化内存分配
  - 管理 CPU 使用
  - 实现负载均衡
  - 优化资源调度
  - 管理并发度
  - 实现资源监控
  - 优化能耗
  - 管理温度控制
  - 实现故障恢复

代码质量:
  - 遵循 CUDA 规范
  - 实现错误检查
  - 优化代码结构
  - 添加完整注释
  - 实现性能分析
  - 优化同步机制
  - 添加单元测试
  - 实现监控日志
```

## 项目概述

### 基本信息
```yaml
项目属性:
  - 应用名称：srt-model-quantizing
  - 开发者：SolidRusT Networks
  - 功能：Hugging Face模型下载、量化和上传管道
  - 设计理念：注重简单性和易用性
  - 硬件兼容：支持NVIDIA CUDA和AMD ROCm GPU
  - 运行平台：仅支持Linux服务器

技术特点:
  - 模型量化处理
  - GPU加速支持
  - 仓库管理集成
  - 自动化部署
```

### 设计原则
```yaml
核心原则:
  - 简单易用：克隆即可运行
  - 依赖管理：最小化安装要求
  - 运行方式：支持Python和Bash
  - 硬件适配：灵活的硬件支持
  - 平台限制：Linux环境专用

实现目标:
  - 流程自动化
  - 错误处理完善
  - 文档更新及时
  - 用户体验优化
```

## 开发规范

### 效率与稳定性
```yaml
效率要求:
  - 优化量化流程
  - 提高处理效率
  - 减少错误发生
  - 资源利用优化

稳定性保证:
  - 处理边缘情况
  - 提供错误信息
  - 建议解决方案
  - 系统健壮性
```

### 文档规范
```yaml
文档要求:
  - README.md维护
  - 使用说明完善
  - 示例代码提供
  - 更新日志记录

内容规范:
  - 清晰的结构
  - 详细的说明
  - 准确的示例
  - 及时的更新
```

## 开发流程

### AI代理对齐
```yaml
开发重点:
  - 保持简单易用
  - 确保代码质量
  - 移除冗余代码
  - 更新文档及时

对齐策略:
  - 进度追踪
  - 优先级管理
  - 目标一致性
  - 开发周期管理
```

### 持续改进
```yaml
改进机制:
  - 收集用户反馈
  - 提出改进建议
  - 优化用户体验
  - 保持核心原则

变更管理:
  - 记录增强功能
  - 修复问题跟踪
  - 变更文档化
  - 确保透明度
```

## 技术实现

### GPU支持
```yaml
NVIDIA支持:
  - CUDA环境配置
  - 驱动程序要求
  - 性能优化
  - 资源管理

AMD支持:
  - ROCm环境配置
  - 硬件兼容性
  - 性能调优
  - 资源控制
```

### 模型处理
```yaml
量化流程:
  - 模型下载
  - 量化处理
  - 结果验证
  - 模型上传

优化策略:
  - 批处理优化
  - 内存管理
  - 并行处理
  - 错误恢复
```

### CUDA 优化
```yaml
核心优化:
  - 内存合并访问
  - 共享内存使用
  - 线程块优化
  - 核函数设计
  - 异步操作
  - 流水线处理

性能调优:
  - 线程配置
  - 内存模式
  - 计算密度
  - 数据局部性
  - 分支优化
  - 资源利用
```

### 量化实现
```python
import torch
import torch.quantization

def quantize_model(model, calibration_data):
    """模型量化实现"""
    # 配置量化参数
    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
    
    # 准备量化
    model_prepared = torch.quantization.prepare(model)
    
    # 校准量化
    with torch.no_grad():
        for data in calibration_data:
            model_prepared(data)
    
    # 完成量化
    model_quantized = torch.quantization.convert(model_prepared)
    
    return model_quantized

def optimize_memory(model, device):
    """内存优化实现"""
    # 使用 CUDA 流
    stream = torch.cuda.Stream(device)
    with torch.cuda.stream(stream):
        # 异步预热
        model(torch.zeros(1, 3, 224, 224).to(device))
    
    # 清理缓存
    torch.cuda.empty_cache()
    
    return model

def parallel_processing(model, data_loader, num_gpus):
    """并行处理实现"""
    # 数据并行
    model = torch.nn.DataParallel(model)
    
    # 批处理并行
    for batch in data_loader:
        # 使用多 GPU 处理
        output = model(batch)
        
    return model
```

### 监控系统
```python
import torch.cuda
import psutil
import time

class ResourceMonitor:
    """资源监控实现"""
    def __init__(self):
        self.gpu_usage = []
        self.memory_usage = []
        self.cpu_usage = []
        
    def collect_metrics(self):
        """收集性能指标"""
        # GPU 使用率
        gpu_usage = torch.cuda.utilization()
        self.gpu_usage.append(gpu_usage)
        
        # 显存使用
        memory_usage = torch.cuda.memory_allocated()
        self.memory_usage.append(memory_usage)
        
        # CPU 使用率
        cpu_usage = psutil.cpu_percent()
        self.cpu_usage.append(cpu_usage)
        
    def generate_report(self):
        """生成性能报告"""
        return {
            'gpu_usage_avg': sum(self.gpu_usage) / len(self.gpu_usage),
            'memory_usage_max': max(self.memory_usage),
            'cpu_usage_avg': sum(self.cpu_usage) / len(self.cpu_usage)
        }

class PerformanceProfiler:
    """性能分析实现"""
    def __init__(self):
        self.start_time = None
        self.end_time = None
        self.events = []
        
    def start(self):
        """开始性能分析"""
        torch.cuda.synchronize()
        self.start_time = time.time()
        
    def record_event(self, name):
        """记录事件"""
        torch.cuda.synchronize()
        self.events.append({
            'name': name,
            'time': time.time() - self.start_time
        })
        
    def end(self):
        """结束性能分析"""
        torch.cuda.synchronize()
        self.end_time = time.time()
        
    def get_profile(self):
        """获取性能分析结果"""
        return {
            'total_time': self.end_time - self.start_time,
            'events': self.events
        }
```

### 部署优化
```python
import torch.onnx
import onnxruntime

def export_model(model, input_shape, save_path):
    """模型导出实现"""
    # 创建示例输入
    dummy_input = torch.randn(input_shape)
    
    # 导出 ONNX 模型
    torch.onnx.export(
        model,
        dummy_input,
        save_path,
        opset_version=11,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )
    
    return save_path

def optimize_inference(model_path):
    """推理优化实现"""
    # 创建推理会话
    session_options = onnxruntime.SessionOptions()
    session_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
    session_options.intra_op_num_threads = 4
    
    # 加载优化后的模型
    session = onnxruntime.InferenceSession(
        model_path,
        session_options,
        providers=['CUDAExecutionProvider']
    )
    
    return session

def deploy_model(model, config):
    """部署模型实现"""
    # 模型优化
    model = optimize_memory(model, config['device'])
    
    # 导出模型
    model_path = export_model(
        model,
        config['input_shape'],
        config['save_path']
    )
    
    # 优化推理
    session = optimize_inference(model_path)
    
    return session
```

## 最佳实践

### 性能优化
```yaml
优化策略:
  - 使用 cudnn benchmark
  - 优化批处理大小
  - 使用混合精度训练
  - 实现梯度累积
  - 优化数据加载
  - 使用 JIT 编译
  - 实现内存规划
  - 优化算子融合

监控指标:
  - GPU 利用率
  - 显存使用率
  - 计算吞吐量
  - 数据传输速度
  - 内存使用情况
  - 温度监控
  - 能耗监控
  - 错误统计
```

### 测试规范
```yaml
测试类型:
  - 性能测试
  - 精度测试
  - 稳定性测试
  - 内存测试
  - 并发测试
  - 负载测试
  - 容错测试
  - 长稳测试

测试工具:
  - CUDA Profiler
  - NSight Systems
  - PyTorch Profiler
  - Memory Profiler
  - nvprof
  - CUPTI
  - Tensor Board
  - 自定义监控
```

## 质量保证

### 测试规范
```yaml
测试要求:
  - 功能测试
  - 性能测试
  - 兼容性测试
  - 稳定性测试

验证流程:
  - 单元测试
  - 集成测试
  - 系统测试
  - 回归测试
```

### 维护更新
```yaml
维护职责:
  - 代码维护
  - 文档更新
  - 问题修复
  - 性能优化

更新策略:
  - 版本控制
  - 特性更新
  - 安全补丁
  - 依赖管理
``` 